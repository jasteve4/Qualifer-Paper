\documentclass[../main.tex]{subfiles}

\begin{document}
\section{CHIPs Architecture}
The CHIPS architecture is an amalgamation of chiplets connected through an interposer. This structure is known as two and a half Dimensions (2.5D), and this structure is the foundation of the CHIPS architecture. The technology that allows chiplets to communication through an interposer is (AIB) advanced interface bus\cite{AIBWhitePaper}. This bus severs as the physical layer in the protocol stack. All chiplets use AXI as the next layer in the protocol stack. AXI is the protocol that makes up the buck of the SOC for the CHIPS architecture.

\subsection{2.5D Structure}
For most of the connects in the 2.5D structure, an AIB driver makes a face to face connection with the interposer. Each AIB driver makes it possible to transfer data at high speeds. AIB drivers connect in pairs through the interposer. One driver needs to be set to send, and the other needs to be set to receive. Figure ?? shows an overview of an AIB driver. 

An AIB channel consists of 48 AIB drivers.  The lower 40 driver transport data and the upper eight drivers transport the clock. The clock drivers transmit in different pairs. These four clocks make up the clock forwarding between chiplets. Clock forwarding allows the chiplets to be independent of each chiplet's clock phase. Figure ?? shows the layout of an AIB channel.

\subsection{CHIPS SOC}
For phase one, there is no interposer. Therefore, AIB drivers connect through standard routing. AIB drivers stack needs to support both a top-level connection and a lower metal connection.  Because of limited area in phase one, AIB wraps around only one of the Rocket core chips. Therefore, each AIB driver on the Rocker core side has to have a corresponding AIB driver on the crosspoint side.

The CHIPs SOC connects chiplets through a support structure. This structure consists of one on-chip memory bus (MBus), one off-chip memory bus (ExMBus), one control bus (XBus), and five debug buses.  Through the XBus, the Rocket cores gain access to the neural network chiplets. The MBus provides access to the on-chip and off-chip memory controller for all of the chiplets. The ExMBus connects off-chip memory to the off-chip memory controller.  Both of the on-chip buses, XBus and MBus, are AXI based, and the off-chip memory bus, ExMBus, is CIPI based. 

\subsection{Chiplets}
Each chiplet has three bus connections. For controller chiplets, Rocket cores, it has two master AXI ports, XBus and MBus, and one Jtag debug port. For the neural network chiplets, it has one master AXI port, MBus, one slave AXI port, XBus, and one Jtag debug port.

%The controllers chiplets use the Rocket Chip IP\cite{AsanoviÄ‡:EECS-2016-17}. In this layout, there are two different layouts for the Rocket Chip, one was wrapped with AIB and the other was not. The two accelerators chiplets are LSTM and SCNN. Figure ?? shows the layout of the tape-out. Each major Design is hi highlighted. 
\subsubsection{Rocket Core}
The Rocket core had to be modified to work in the CHIPs architecture. In this version of the Rocket core, there is no floating-point hardware, and there is support for the custom instructions extensions. Only one of the custom instruction extensions is used to control access to the XBus. This instruction has two variants, bus read and bus write. These two actions allow the Rocket core to control and debug the neural network chiplets via XBus. See figure ?? for the layout of this version of the Rocket core. Figure ?? shows the layout of the new Rocket chiplet.
\subsubsection{Neural Network Chiplets}
In this architecture, there are two neural network chiplets: LSTM and SCNN. Both chiplets are accessible via Jtag port or XBus port. These chiplets are designed to work at the coarse grain programming level. 

The first chiplet of interest is the long short term chiplet (LSTM). At NCSU, Dr. Summon Dey designed this chiplet for his Ph.D. The following is an overview of his design. His design focuses on developing hardware that can fully utilization memory bandwidth. His design is scalable with the amount of memory bandwidth available. In addition to being scalable, it needed to be sensitive changes in recurrent neural networks (RNN) algorithms. To this end, he based his design on a very long instruction word (VLIW) architecture. In this style of architecture, a single instruction represents a set to micro-opts. Each micro-opt controls a section/lane of the design. See his paper for more details on his design\cite{Summon-Dey-LSTM}. 

The second chiplet of interest is the sparse convolution neural network (SCNN). At NCSU, Dr designed this chiplet for his PH.D. The following is an overview of his design. His design reduces the amount of memory access needed to process a neural network. In his design, he compressed the weight and data in memory by using a run-length encoding (RLC) format. His design is optimized to handle the gaps in data and weights to perform processing on a CNN. Therefore, the data and weights are densely packed. His design is not memory bound, but computation bound. See his section on ASIP in his dissertation\cite{LeWeifuDissertation}.

\end{document}

